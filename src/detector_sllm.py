"""
sLLM ê¸°ë°˜ ìš•ì„¤/í­ì–¸ ê°ì§€ ì—”ì§„
GGUF í˜•ì‹ ëª¨ë¸ ì‚¬ìš© (llama.cpp)
"""

import time
import os
from typing import Dict, Any, List


class SLLMAbusiveDetector:
    """
    sLLM ê¸°ë°˜ ìš•ì„¤/í­ì–¸ ê°ì§€ ì—”ì§„
    Midm-2.0-Mini-Instruct 4B ëª¨ë¸ ì‚¬ìš©
    """
    
    def __init__(self,
                 model_path: str = "./models/Midm-2.0-Mini-Instruct-Q4_K_M.gguf",
                 threshold: float = 0.5,
                 n_ctx: int = 2048,
                 n_threads: int = None,
                 verbose: bool = False):
        """
        Args:
            model_path: GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            threshold: ê°ì§€ ì„ê³„ê°’
            n_ctx: ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            n_threads: CPU ìŠ¤ë ˆë“œ ìˆ˜ (None=ìë™)
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        """
        self.model_path = model_path
        self.threshold = threshold
        self.n_ctx = n_ctx
        self.n_threads = n_threads or os.cpu_count()
        self.verbose = verbose
        
        self.llm = None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.system_prompt = """ë‹¹ì‹ ì€ ê³ ê° ì„œë¹„ìŠ¤ í’ˆì§ˆ ê´€ë¦¬ ë° í†µí™” ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í†µí™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë¶€ì ì ˆí•œ ë°œì–¸ì´ ìˆëŠ”ì§€ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ã€í‰ê°€ ëŒ€ìƒã€‘
1. ìš•ì„¤/ë¹„ì†ì–´: ëª…ì‹œì ì¸ ìš•ì„¤ ë° ë¹„ì†ì–´ ì‚¬ìš©
2. ëª¨ìš•/í­ì–¸: ìƒëŒ€ë°©ì„ ê¹ì•„ë‚´ë¦¬ê±°ë‚˜ ê³µê²©í•˜ëŠ” í‘œí˜„
3. ì„±í¬ë¡±: ì™¸ëª¨ í‰ê°€, ì„±ì  ì œì•ˆ, ê°œì¸ì  ì§ˆë¬¸ ë“±
4. ìœ„í˜‘: í˜‘ë°•, ìœ„í˜‘ì  í‘œí˜„

ã€í‰ê°€ ë°©ë²•ã€‘
- ëª…í™•í•œ í‘œí˜„ë¿ë§Œ ì•„ë‹ˆë¼ ë¬¸ë§¥ê³¼ ì˜ë„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨
- ê°„ì ‘ì ì´ê±°ë‚˜ ìš°íšŒì ì¸ í‘œí˜„ë„ ì˜ë„ê°€ ëª…í™•í•˜ë©´ ê°ì§€
- ë‹¨ì–´ë§Œ ë³´ì§€ ë§ê³  ì „ì²´ ë§¥ë½ì—ì„œ ë°œí™”ìì˜ ì˜ë„ íŒŒì•…
- ì •ìƒì ì¸ ë¶ˆë§Œì´ë‚˜ ê±´ì˜ëŠ” ë¶€ì ì ˆí•˜ì§€ ì•ŠìŒ

ã€ì •ìƒ vs ë¶€ì ì ˆ êµ¬ë¶„ã€‘
ì •ìƒì  í‘œí˜„:
- "ë¶ˆí¸í•©ë‹ˆë‹¤", "ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤", "ë‹µë‹µí•˜ë„¤ìš”"
- "ë¹ ë¥¸ ì²˜ë¦¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤", "ì‹¤ë§ìŠ¤ëŸ½ìŠµë‹ˆë‹¤"

ë¶€ì ì ˆí•œ í‘œí˜„:
- ì§ì ‘ì  ìš•ì„¤: "ì”¨ë°œ", "ë³‘ì‹ ", "ê°œìƒˆë¼" ë“±
- ëª¨ìš•ì  í‘œí˜„: "ì“°ë ˆê¸°", "ì¸ê°„ ë§ì¢…", "ë¯¸ì¹œë†ˆ" ë“±
- ì„±í¬ë¡±: "ì˜ˆì˜ë„¤ìš”", "ë‚¨ìì¹œêµ¬ ìˆì–´ìš”?", "ëª¸ë§¤ ì¢‹ìœ¼ì‹œë„¤ìš”" ë“±
- ìœ„í˜‘: "ì£½ì´ê³  ì‹¶ë‹¤", "ê°€ë§Œ ì•ˆ ë‘”ë‹¤" ë“±

ã€ì±„ì  ê¸°ì¤€ã€‘
0.0-0.2: ì „í˜€ ë¶€ì ì ˆí•˜ì§€ ì•ŠìŒ (ì •ìƒì ì¸ ëŒ€í™”)
0.3-0.4: ì•½ê°„ ê±°ì¹œ í‘œí˜„ì´ë‚˜ ë¶ˆë§Œ (ê²½ê³„ì„ )
0.5-0.6: ë¶€ì ì ˆí•œ ì˜ë„ê°€ ìˆìœ¼ë‚˜ ê²½ë¯¸í•¨ (ì£¼ì˜ í•„ìš”)
0.7-0.8: ëª…ë°±íˆ ë¶€ì ì ˆí•¨ (ê²½ê³ )
0.9-1.0: ë§¤ìš° ì‹¬ê°í•œ ë¶€ì ì ˆí•¨ (ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”)

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
ì ìˆ˜: [0.0~1.0 ì‚¬ì´ ìˆ«ì]
íŒë‹¨: [ë¶€ì ì ˆ/ì •ìƒ]
ì¹´í…Œê³ ë¦¬: [ìš•ì„¤/ëª¨ìš•/ì„±í¬ë¡±/ìœ„í˜‘/ë³µí•©/ì—†ìŒ]
ì´ìœ : [ë¬¸ë§¥ê³¼ ì˜ë„ë¥¼ ê³ ë ¤í•œ êµ¬ì²´ì  íŒë‹¨ ê·¼ê±°]"""
        
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        if self.llm is None:
            try:
                from llama_cpp import Llama
            except ImportError:
                raise ImportError(
                    "llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                    "ì„¤ì¹˜: pip install llama-cpp-python"
                )
            
            if not os.path.exists(self.model_path):
                raise FileNotFoundError(
                    f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}\n"
                    "models/ í´ë”ì— GGUF ëª¨ë¸ì„ ë°°ì¹˜í•˜ì„¸ìš”."
                )
            
            print(f"\n{'='*60}")
            print("ğŸ¤– sLLM ëª¨ë¸ ë¡œë”© ì¤‘...")
            print(f"{'='*60}\n")
            print(f"ğŸ“¦ ëª¨ë¸: {os.path.basename(self.model_path)}")
            print(f"ğŸ§µ ìŠ¤ë ˆë“œ: {self.n_threads}")
            print(f"ğŸ“ ì»¨í…ìŠ¤íŠ¸: {self.n_ctx}")
            print()
            
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=self.n_ctx,
                n_threads=self.n_threads,
                verbose=self.verbose,
                n_gpu_layers=0  # CPU only (GPU ì‚¬ìš© ì‹œ ê°’ ì¡°ì •)
            )
            
            print(f"\n{'='*60}")
            print("âœ… sLLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            print(f"{'='*60}\n")
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """
        LLM ì‘ë‹µ íŒŒì‹±
        
        ì˜ˆìƒ í˜•ì‹:
        ì ìˆ˜: 0.85
        íŒë‹¨: ë¶€ì ì ˆ
        ì¹´í…Œê³ ë¦¬: ìš•ì„¤
        ì´ìœ : "ì”¨ë°œ", "ë³‘ì‹ " ë“±ì˜ ìš•ì„¤ í¬í•¨
        """
        lines = response.strip().split('\n')
        
        score = 0.5
        is_abusive = False
        category = "ì—†ìŒ"
        reason = ""
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('ì ìˆ˜:'):
                try:
                    score_str = line.split(':')[1].strip()
                    score = float(score_str)
                    # ì ìˆ˜ê°€ 0.5 ì´ìƒì´ë©´ ë¶€ì ì ˆë¡œ íŒë‹¨
                    is_abusive = score >= self.threshold
                except:
                    pass
            
            elif line.startswith('íŒë‹¨:'):
                judgment = line.split(':')[1].strip()
                # "ë¶€ì ì ˆ" ë˜ëŠ” "ìš•ì„¤ ìˆìŒ" ë“±ì˜ í‘œí˜„ ê°ì§€
                is_abusive = any(keyword in judgment for keyword in ['ë¶€ì ì ˆ', 'ìˆìŒ', 'ê°ì§€', 'ë°œê²¬'])
            
            elif line.startswith('ì¹´í…Œê³ ë¦¬:'):
                category = line.split(':', 1)[1].strip()
            
            elif line.startswith('ì´ìœ :'):
                reason = line.split(':', 1)[1].strip()
        
        return {
            'score': score,
            'is_abusive': is_abusive,
            'category': category,
            'reason': reason
        }
    
    def predict(self, text: str) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ë¶„ì„
        """
        if self.llm is None:
            self.load_model()
        
        start_time = time.time()
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""<|im_start|>system
{self.system_prompt}<|im_end|>
<|im_start|>user
ë‹¤ìŒ í†µí™” ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.
ëª…ì‹œì  í‘œí˜„ë¿ë§Œ ì•„ë‹ˆë¼ ë¬¸ë§¥ê³¼ ì˜ë„ë¥¼ ê¹Šì´ íŒŒì•…í•˜ì—¬ ì ìˆ˜ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”.

í†µí™” ë‚´ìš©:
"{text}"

ìœ„ ë‚´ìš©ì—ì„œ:
1. ìš•ì„¤, ëª¨ìš•, í­ì–¸, ì„±í¬ë¡± ë“±ì˜ ë¶€ì ì ˆí•œ í‘œí˜„ì´ ìˆë‚˜ìš”?
2. ì§ì ‘ì  í‘œí˜„ì´ ì—†ë”ë¼ë„ ê·¸ëŸ¬í•œ ì˜ë„ê°€ ë‹´ê²¨ìˆë‚˜ìš”?
3. ì „ì²´ ë§¥ë½ì—ì„œ ë°œí™”ìì˜ íƒœë„ì™€ ì˜ë„ëŠ” ì–´ë–¤ê°€ìš”?
<|im_end|>
<|im_start|>assistant
"""
        
        # LLM ì¶”ë¡ 
        response = self.llm(
            prompt,
            max_tokens=256,
            temperature=0.1,  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± í™•ë³´
            top_p=0.9,
            stop=["<|im_end|>", "\n\n\n"],
            echo=False
        )
        
        response_text = response['choices'][0]['text'].strip()
        
        # ì‘ë‹µ íŒŒì‹±
        parsed = self._parse_response(response_text)
        
        processing_time = time.time() - start_time
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "text": text,
            "is_abusive": parsed['is_abusive'] or parsed['score'] >= self.threshold,
            "confidence": 1.0 - abs(parsed['score'] - 0.5) * 2,  # 0.5ì—ì„œ ë©€ìˆ˜ë¡ í™•ì‹ 
            "abusive_score": parsed['score'],
            "category": parsed.get('category', 'ì—†ìŒ'),  # ì¹´í…Œê³ ë¦¬ ì¶”ê°€
            "threshold": self.threshold,
            "processing_time": processing_time,
            "model_type": "sLLM",
            "model_name": os.path.basename(self.model_path),
            "reason": parsed['reason'],
            "raw_response": response_text
        }
        
        return result
    
    def predict_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì˜ˆì¸¡"""
        results = []
        for text in texts:
            result = self.predict(text)
            results.append(result)
        return results
    
    def predict_file(self, filepath: str) -> Dict[str, Any]:
        """íŒŒì¼ì—ì„œ ì½ì–´ì„œ ì˜ˆì¸¡"""
        from .preprocessor import TextPreprocessor
        
        preprocessor = TextPreprocessor()
        text = preprocessor.preprocess_file(filepath)
        
        result = self.predict(text)
        result["source_file"] = filepath
        
        return result
    
    def __del__(self):
        """ì†Œë©¸ì - ëª¨ë¸ ì •ë¦¬"""
        if self.llm is not None:
            del self.llm
