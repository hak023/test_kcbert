"""
sLLM ê¸°ë°˜ ìš•ì„¤/í­ì–¸ ê°ì§€ ì—”ì§„
GGUF í˜•ì‹ ëª¨ë¸ ì‚¬ìš© (llama.cpp)
"""

import time
import os
from typing import Dict, Any, List


class SLLMAbusiveDetector:
    """
    sLLM ê¸°ë°˜ ìš•ì„¤/í­ì–¸ ê°ì§€ ì—”ì§„
    Midm-2.0-Mini-Instruct 4B ëª¨ë¸ ì‚¬ìš©
    """
    
    def __init__(self,
                 model_path: str = "./models/Midm-2.0-Mini-Instruct-Q4_K_M.gguf",
                 threshold: float = 0.5,
                 n_ctx: int = 2048,
                 n_threads: int = None,
                 verbose: bool = False):
        """
        Args:
            model_path: GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            threshold: ê°ì§€ ì„ê³„ê°’
            n_ctx: ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            n_threads: CPU ìŠ¤ë ˆë“œ ìˆ˜ (None=ìë™)
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        """
        self.model_path = model_path
        self.threshold = threshold
        self.n_ctx = n_ctx
        self.n_threads = n_threads or os.cpu_count()
        self.verbose = verbose
        
        self.llm = None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.system_prompt = """ë‹¹ì‹ ì€ ê³ ê° ì„œë¹„ìŠ¤ í’ˆì§ˆ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í†µí™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìš•ì„¤, í­ì–¸, ê³µê²©ì ì¸ ì–¸ì–´ê°€ ìˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- ìš•ì„¤/ë¹„ì†ì–´ ì‚¬ìš© (ì”¨ë°œ, ë³‘ì‹ , ê°œìƒˆë¼ ë“±)
- ìœ„í˜‘ì  í‘œí˜„ (ì£½ì´ê³  ì‹¶ë‹¤, ë•Œë¦¬ê³  ì‹¶ë‹¤ ë“±)
- ì‹¬í•œ ëª¨ìš• (ì“°ë ˆê¸°, ì¸ê°„ë§ì¢… ë“±)

ì •ìƒì ì¸ ë¶ˆë§Œ í‘œí˜„ì€ ìš•ì„¤ì´ ì•„ë‹™ë‹ˆë‹¤:
- "ë‹µë‹µí•˜ë„¤ìš”", "ë¶ˆí¸í•©ë‹ˆë‹¤", "ê°œì„  í•„ìš”í•©ë‹ˆë‹¤" ë“±

ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
ì ìˆ˜: [0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ìˆ«ì]
íŒë‹¨: [ìš•ì„¤ ìˆìŒ/ìš•ì„¤ ì—†ìŒ]
ì´ìœ : [ê°„ë‹¨í•œ ì´ìœ ]"""
        
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        if self.llm is None:
            try:
                from llama_cpp import Llama
            except ImportError:
                raise ImportError(
                    "llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                    "ì„¤ì¹˜: pip install llama-cpp-python"
                )
            
            if not os.path.exists(self.model_path):
                raise FileNotFoundError(
                    f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}\n"
                    "models/ í´ë”ì— GGUF ëª¨ë¸ì„ ë°°ì¹˜í•˜ì„¸ìš”."
                )
            
            print(f"\n{'='*60}")
            print("ğŸ¤– sLLM ëª¨ë¸ ë¡œë”© ì¤‘...")
            print(f"{'='*60}\n")
            print(f"ğŸ“¦ ëª¨ë¸: {os.path.basename(self.model_path)}")
            print(f"ğŸ§µ ìŠ¤ë ˆë“œ: {self.n_threads}")
            print(f"ğŸ“ ì»¨í…ìŠ¤íŠ¸: {self.n_ctx}")
            print()
            
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=self.n_ctx,
                n_threads=self.n_threads,
                verbose=self.verbose,
                n_gpu_layers=0  # CPU only (GPU ì‚¬ìš© ì‹œ ê°’ ì¡°ì •)
            )
            
            print(f"\n{'='*60}")
            print("âœ… sLLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            print(f"{'='*60}\n")
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """
        LLM ì‘ë‹µ íŒŒì‹±
        
        ì˜ˆìƒ í˜•ì‹:
        ì ìˆ˜: 0.85
        íŒë‹¨: ìš•ì„¤ ìˆìŒ
        ì´ìœ : "ì”¨ë°œ", "ë³‘ì‹ " ë“±ì˜ ìš•ì„¤ í¬í•¨
        """
        lines = response.strip().split('\n')
        
        score = 0.5
        is_abusive = False
        reason = ""
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('ì ìˆ˜:'):
                try:
                    score_str = line.split(':')[1].strip()
                    score = float(score_str)
                except:
                    pass
            
            elif line.startswith('íŒë‹¨:'):
                judgment = line.split(':')[1].strip()
                is_abusive = 'ìš•ì„¤ ìˆìŒ' in judgment or 'ìˆìŒ' in judgment
            
            elif line.startswith('ì´ìœ :'):
                reason = line.split(':', 1)[1].strip()
        
        return {
            'score': score,
            'is_abusive': is_abusive,
            'reason': reason
        }
    
    def predict(self, text: str) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ë¶„ì„
        """
        if self.llm is None:
            self.load_model()
        
        start_time = time.time()
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""<|im_start|>system
{self.system_prompt}<|im_end|>
<|im_start|>user
ë‹¤ìŒ í†µí™” ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

"{text}"
<|im_end|>
<|im_start|>assistant
"""
        
        # LLM ì¶”ë¡ 
        response = self.llm(
            prompt,
            max_tokens=256,
            temperature=0.1,  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± í™•ë³´
            top_p=0.9,
            stop=["<|im_end|>", "\n\n\n"],
            echo=False
        )
        
        response_text = response['choices'][0]['text'].strip()
        
        # ì‘ë‹µ íŒŒì‹±
        parsed = self._parse_response(response_text)
        
        processing_time = time.time() - start_time
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "text": text,
            "is_abusive": parsed['is_abusive'] or parsed['score'] >= self.threshold,
            "confidence": 1.0 - abs(parsed['score'] - 0.5) * 2,  # 0.5ì—ì„œ ë©€ìˆ˜ë¡ í™•ì‹ 
            "abusive_score": parsed['score'],
            "threshold": self.threshold,
            "processing_time": processing_time,
            "model_type": "sLLM",
            "model_name": os.path.basename(self.model_path),
            "reason": parsed['reason'],
            "raw_response": response_text
        }
        
        return result
    
    def predict_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì˜ˆì¸¡"""
        results = []
        for text in texts:
            result = self.predict(text)
            results.append(result)
        return results
    
    def predict_file(self, filepath: str) -> Dict[str, Any]:
        """íŒŒì¼ì—ì„œ ì½ì–´ì„œ ì˜ˆì¸¡"""
        from .preprocessor import TextPreprocessor
        
        preprocessor = TextPreprocessor()
        text = preprocessor.preprocess_file(filepath)
        
        result = self.predict(text)
        result["source_file"] = filepath
        
        return result
    
    def __del__(self):
        """ì†Œë©¸ì - ëª¨ë¸ ì •ë¦¬"""
        if self.llm is not None:
            del self.llm
