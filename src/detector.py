"""
ìš•ì„¤/í­ì–¸ ê°ì§€ ì—”ì§„ ëª¨ë“ˆ
"""

import time
import torch
import numpy as np
from typing import Dict, List, Any
from .model_loader import ModelLoader


class AbusiveDetector:
    """KcBERT ê¸°ë°˜ ìš•ì„¤/í­ì–¸ ê°ì§€ ì—”ì§„"""
    
    def __init__(self,
                 model_name: str = "beomi/kcbert-base",
                 cache_dir: str = "./models/kcbert",
                 threshold: float = 0.5,
                 max_length: int = 300):  # KcBERT ìµœëŒ€ ê¸¸ì´ëŠ” 300
        """
        Args:
            model_name: ëª¨ë¸ëª…
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
            threshold: ê°ì§€ ì„ê³„ê°’ (0.0 ~ 1.0)
            max_length: ìµœëŒ€ í† í° ê¸¸ì´ (KcBERTëŠ” 300ì´ ìµœëŒ€)
        """
        self.threshold = threshold
        self.max_length = max_length
        
        # ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”
        self.loader = ModelLoader(
            model_name=model_name,
            cache_dir=cache_dir
        )
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” ì§€ì—° ë¡œë”©
        self.tokenizer = None
        self.model = None
        self.device = None
        
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ìš•ì„¤ íŒ¨í„´ (ë³´ì¡° ê¸°ëŠ¥)
        # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì‚¬ì „ì´ í•„ìš”í•˜ì§€ë§Œ, ì˜ˆì œìš©ìœ¼ë¡œ ê°„ë‹¨íˆ êµ¬ì„±
        self.abusive_patterns = [
            'ì‹œë°œ', 'ì”¨ë°œ', 'ë³‘ì‹ ', 'ê°œìƒˆ', 'ì¢†', 'ë‹ˆë¯¸', 
            'ì§€ë„', 'ì—¿ë¨¹', 'êº¼ì ¸', 'ã……ã…‚', 'ã…‚ã……', 'ë¯¸ì¹œ'
        ]
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ (ì§€ì—° ë¡œë”©)"""
        if self.model is None:
            print("\n" + "="*60)
            print("ğŸ¤– KcBERT ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            print("="*60 + "\n")
            
            self.tokenizer, self.model = self.loader.load()
            self.device = self.loader.get_device()
            
            print("\n" + "="*60)
            print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
            print("="*60 + "\n")
    
    def _check_rule_based(self, text: str) -> float:
        """
        ê·œì¹™ ê¸°ë°˜ ìš•ì„¤ ì²´í¬ (ë³´ì¡° ê¸°ëŠ¥)
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ (0.0 ~ 1.0)
        """
        text_lower = text.lower()
        matches = sum(1 for pattern in self.abusive_patterns if pattern in text_lower)
        
        # ë§¤ì¹­ëœ íŒ¨í„´ ìˆ˜ì— ë”°ë¼ ì ìˆ˜ ê³„ì‚°
        if matches == 0:
            return 0.0
        elif matches == 1:
            return 0.6
        elif matches == 2:
            return 0.8
        else:
            return 0.95
    
    def predict(self, text: str) -> Dict[str, Any]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            ê°ì§€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ëª¨ë¸ ë¡œë“œ (ì²˜ìŒ í˜¸ì¶œ ì‹œ)
        if self.model is None:
            self.load_model()
        
        start_time = time.time()
        
        # í† í°í™”
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=self.max_length,
            padding="max_length",
            truncation=True
        )
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ì¶”ë¡ 
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            
            # Softmaxë¡œ í™•ë¥  ê³„ì‚°
            probabilities = torch.nn.functional.softmax(logits, dim=-1)
            abusive_prob = probabilities[0][1].item()  # ìš•ì„¤ í´ë˜ìŠ¤ í™•ë¥ 
            confidence = torch.max(probabilities).item()
        
        # ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ì™€ ê²°í•©
        rule_score = self._check_rule_based(text)
        
        # ìµœì¢… ì ìˆ˜ = (ëª¨ë¸ ì ìˆ˜ * 0.7) + (ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ * 0.3)
        # ëª¨ë¸ì´ ì œëŒ€ë¡œ fine-tuningë˜ì§€ ì•Šì€ ê²½ìš° ê·œì¹™ ê¸°ë°˜ì— ë” ì˜ì¡´
        if rule_score > 0.5:
            final_score = max(abusive_prob, rule_score)
        else:
            final_score = abusive_prob * 0.7 + rule_score * 0.3
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        processing_time = time.time() - start_time
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "text": text,
            "is_abusive": final_score >= self.threshold,
            "confidence": confidence,
            "abusive_score": final_score,
            "model_score": abusive_prob,
            "rule_score": rule_score,
            "threshold": self.threshold,
            "processing_time": processing_time
        }
        
        return result
    
    def predict_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ ì˜ˆì¸¡
        
        Args:
            texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ê°ì§€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for text in texts:
            result = self.predict(text)
            results.append(result)
        
        return results
    
    def predict_file(self, filepath: str) -> Dict[str, Any]:
        """
        íŒŒì¼ì—ì„œ ì½ì–´ì„œ ì˜ˆì¸¡
        
        Args:
            filepath: í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ê°ì§€ ê²°ê³¼
        """
        from .preprocessor import TextPreprocessor
        
        preprocessor = TextPreprocessor()
        text = preprocessor.preprocess_file(filepath)
        
        result = self.predict(text)
        result["source_file"] = filepath
        
        return result
