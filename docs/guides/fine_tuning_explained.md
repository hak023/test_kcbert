# Fine-tuning 완벽 가이드

## 📋 목차

1. [Fine-tuning이란?](#fine-tuning이란)
2. [모델이 변경되는 과정](#모델이-변경되는-과정)
3. [기존 지식과의 관계](#기존-지식과의-관계)
4. [실제 예시로 이해하기](#실제-예시로-이해하기)
5. [장단점](#장단점)
6. [KcBERT Fine-tuning 방법](#kcbert-fine-tuning-방법)

---

## Fine-tuning이란?

### 📚 정의

**Fine-tuning(파인튜닝)** = 사전 학습된 모델을 특정 작업에 맞게 **추가 학습**시키는 것

```
사전 학습 모델 (Pre-trained Model)
         ↓
    + 추가 학습 데이터
         ↓
  Fine-tuned 모델 (특화 모델)
```

### 🎯 핵심 개념

**네, 맞습니다! 모델에 추가 학습을 시켜서 모델의 가중치(weights)가 변경됩니다.**

---

## 모델이 변경되는 과정

### 1️⃣ 모델의 구조

```
┌─────────────────────────────────────────┐
│         KcBERT 모델 구조                │
├─────────────────────────────────────────┤
│                                         │
│  [입력층] 토크나이저                     │
│     ↓                                   │
│  [BERT 층] 12개의 트랜스포머 레이어      │
│     - Layer 1: 가중치 W1, 편향 b1       │
│     - Layer 2: 가중치 W2, 편향 b2       │
│     - ...                               │
│     - Layer 12: 가중치 W12, 편향 b12    │
│     ↓                                   │
│  [출력층] 분류 레이어                    │
│     - 가중치 Wc, 편향 bc                │
│     ↓                                   │
│  [결과] 욕설/정상 (현재)                 │
└─────────────────────────────────────────┘
```

### 2️⃣ Fine-tuning 전후 비교

#### **Fine-tuning 전 (현재)**

```python
# 모델 파일: model.safetensors
{
  "bert.layer.0.weight": [0.123, -0.456, 0.789, ...],  # 일반적인 한국어 이해
  "bert.layer.1.weight": [0.234, -0.567, 0.890, ...],  # 일반적인 문맥 파악
  ...
  "classifier.weight": [0.345, -0.678, 0.901, ...],    # 랜덤 초기화 (욕설 학습 X)
  "classifier.bias": [0.5, 0.5]                        # 기본값
}

# 예측 결과
"이 예쁜 것아, 가슴 만져도 돼?" 
→ 욕설 점수: 0.31 (낮음, 부정확)
→ 이유: 성희롱에 대한 학습이 없어서 판단 못함
```

#### **Fine-tuning 후 (학습 완료)**

```python
# 모델 파일: model_finetuned.safetensors
{
  "bert.layer.0.weight": [0.125, -0.458, 0.791, ...],  # 약간 변경 (한국어 지식 유지)
  "bert.layer.1.weight": [0.236, -0.569, 0.892, ...],  # 약간 변경
  ...
  "bert.layer.11.weight": [0.250, -0.600, 0.920, ...], # 더 많이 변경 (문맥 특화)
  "classifier.weight": [0.890, -0.234, 0.567, ...],    # 크게 변경 (욕설/성희롱 학습됨!)
  "classifier.bias": [0.1, 0.9]                        # 최적화됨
}

# 예측 결과
"이 예쁜 것아, 가슴 만져도 돼?" 
→ 욕설/성희롱 점수: 0.92 (높음, 정확!)
→ 이유: 성희롱 패턴을 학습했음
```

### 3️⃣ 변경되는 것들

| 항목 | 변경 여부 | 변경 정도 | 설명 |
|-----|---------|---------|------|
| **모델 구조** | ❌ 변경 안됨 | - | 레이어 개수, 크기 동일 |
| **토크나이저** | ❌ 변경 안됨 | - | 단어 분리 방식 동일 |
| **하위 레이어 가중치** | ✅ 약간 변경 | 1~5% | 한국어 기본 지식 유지 |
| **상위 레이어 가중치** | ✅ 많이 변경 | 10~30% | 문맥 이해 특화 |
| **출력층 가중치** | ✅ 크게 변경 | 50~100% | 욕설/성희롱 판단 특화 |

---

## 기존 지식과의 관계

### 🧠 사람의 학습과 비교

```
┌──────────────────────────────────────────────────────────┐
│  사람의 학습                                              │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  [초등학교] 한글, 기본 어휘, 문법 학습                     │
│      ↓                                                   │
│  [중고등학교] 다양한 문장, 표현 학습                       │
│      ↓                                                   │
│  [대학교] 일반적인 지식                                   │
│      ↓                                                   │
│  [직장 교육] 고객센터 상담 전문 교육 ← Fine-tuning!       │
│      - 욕설 패턴 학습                                    │
│      - 성희롱 표현 학습                                   │
│      - 응대 방법 학습                                    │
│      ↓                                                   │
│  [전문가] 고객센터 전문 상담사                            │
│                                                          │
└──────────────────────────────────────────────────────────┘

특징:
✅ 기존 지식(한글, 문법)은 유지
✅ 새로운 전문 지식(욕설, 성희롱 판단) 추가
✅ 특정 작업(고객센터 분석)에 특화
```

### 🤖 모델의 학습

```
┌──────────────────────────────────────────────────────────┐
│  KcBERT의 학습                                            │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  [사전 학습] 대량의 한국어 텍스트                          │
│   - 뉴스, 블로그, SNS 등 21GB                            │
│   - 한국어 어휘, 문법, 문맥 이해                          │
│   - 감정, 뉘앙스 파악                                    │
│      ↓                                                   │
│  [Fine-tuning] 욕설/성희롱 데이터 ← 여기서 추가 학습!     │
│   - 욕설 문장 1,000개                                    │
│   - 성희롱 문장 1,000개                                  │
│   - 정상 문장 1,000개                                    │
│      ↓                                                   │
│  [특화 모델] 욕설/성희롱 전문 감지 모델                    │
│                                                          │
└──────────────────────────────────────────────────────────┘

특징:
✅ 한국어 이해력 유지 (사전 학습 지식)
✅ 욕설/성희롱 판단 능력 추가 (Fine-tuning)
✅ 소량의 데이터로도 효과적 (전이 학습)
```

---

## 실제 예시로 이해하기

### 📊 Fine-tuning 전후 비교

#### **테스트 문장들**

| 문장 | Fine-tuning 전 | Fine-tuning 후 | 설명 |
|-----|---------------|---------------|------|
| "배송이 늦어서 불편합니다" | ✅ 정상 (0.31) | ✅ 정상 (0.15) | 더 확신 있게 정상 판단 |
| "씨발 빨리 보내라" | ⚠️ 의심 (0.60) | ✅ 욕설 (0.95) | 명확히 욕설 감지 |
| "예쁘네요, 남자친구 있어요?" | ❌ 정상 (0.31) | ✅ 성희롱 (0.87) | 성희롱 정확히 감지 |
| "가슴 만져도 돼?" | ❌ 정상 (0.30) | ✅ 성희롱 (0.98) | 심각한 성희롱 감지 |
| "상담사님 친절하시네요" | ✅ 정상 (0.29) | ✅ 정상 (0.12) | 더 확신 있게 정상 판단 |

#### **정확도 개선**

```
Fine-tuning 전:
├─ 욕설 감지율: 60% (놓치는 경우 많음)
├─ 성희롱 감지율: 10% (거의 못 잡음)
└─ 정상 오판율: 15% (정상을 욕설로 오판)

Fine-tuning 후:
├─ 욕설 감지율: 95% ⬆️ +35%
├─ 성희롱 감지율: 90% ⬆️ +80%
└─ 정상 오판율: 3% ⬇️ -12%
```

---

## 장단점

### ✅ Fine-tuning의 장점

#### 1. **높은 정확도**
```
규칙 기반: 80% 정확도
Fine-tuned 모델: 95% 정확도 ⬆️
```

#### 2. **문맥 이해**
```
규칙 기반: "예쁘다" → 단어만 봄
Fine-tuned: "제품이 예쁘다" (정상) vs "얼굴이 예쁘다" (성희롱) → 문맥 파악
```

#### 3. **우회 표현 감지**
```
규칙 기반: 패턴에 없으면 못 잡음
Fine-tuned: 학습한 패턴 일반화 → 새로운 표현도 감지
```

#### 4. **지속적 개선**
```
데이터 추가 → 재학습 → 정확도 향상
```

### ❌ Fine-tuning의 단점

#### 1. **학습 데이터 필요**
```
최소: 각 클래스당 500~1,000개
권장: 각 클래스당 3,000~5,000개
```

#### 2. **학습 시간/비용**
```
CPU: 4~8시간
GPU: 1~2시간
클라우드 GPU: 비용 발생
```

#### 3. **라벨링 작업**
```
3,000개 문장 라벨링
→ 1인 기준 2~3일 작업
→ 크라우드소싱 시 비용 발생
```

#### 4. **전문 지식 필요**
```
- 학습 파라미터 조정
- 과적합(Overfitting) 방지
- 평가 및 검증
```

---

## KcBERT Fine-tuning 방법

### 📝 필요한 데이터 형식

```python
# train.csv 예시
text,label
"배송이 늦어서 불편합니다",정상
"상담사님 친절하시네요",정상
"씨발 빨리 보내라 개새끼들아",욕설
"미친놈들 진짜 짜증나네",욕설
"예쁘네요 남자친구 있어요?",성희롱
"가슴 만져도 돼?",성희롱
"몸매 좋으신데 같이 저녁 드실래요?",성희롱
...

총 필요 데이터:
- 정상: 1,500개
- 욕설: 1,500개
- 성희롱: 1,500개
────────────────
합계: 4,500개 (최소 3,000개)
```

### 🔧 Fine-tuning 코드 구조

```python
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset
import torch

# 1. 데이터 로드
dataset = load_dataset('csv', data_files={
    'train': 'data/train.csv',
    'validation': 'data/validation.csv',
    'test': 'data/test.csv'
})

# 2. 레이블 매핑
label_mapping = {
    "정상": 0,
    "욕설": 1,
    "성희롱": 2
}

# 3. 모델 로드
model = AutoModelForSequenceClassification.from_pretrained(
    "beomi/kcbert-base",
    num_labels=3,  # 정상, 욕설, 성희롱
    problem_type="single_label_classification"
)

# 4. 학습 설정
training_args = TrainingArguments(
    output_dir='./models/kcbert-finetuned',
    num_train_epochs=3,              # 3번 반복 학습
    per_device_train_batch_size=16,  # 한번에 16개씩 학습
    learning_rate=2e-5,               # 학습률 (작게!)
    warmup_steps=500,                 # 워밍업
    weight_decay=0.01,                # 과적합 방지
    logging_steps=100,                # 로그 주기
    evaluation_strategy="epoch",      # 매 epoch마다 평가
    save_strategy="epoch",            # 매 epoch마다 저장
    load_best_model_at_end=True,     # 최고 성능 모델 선택
)

# 5. 학습 실행
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
)

trainer.train()

# 6. 모델 저장
model.save_pretrained('./models/kcbert-finetuned')
tokenizer.save_pretrained('./models/kcbert-finetuned')
```

### 📊 학습 과정

```
Epoch 1/3:
  Step 100: loss=0.823, accuracy=0.65
  Step 200: loss=0.612, accuracy=0.78
  Step 300: loss=0.445, accuracy=0.85
  Validation: loss=0.398, accuracy=0.87

Epoch 2/3:
  Step 400: loss=0.334, accuracy=0.89
  Step 500: loss=0.287, accuracy=0.91
  Step 600: loss=0.256, accuracy=0.93
  Validation: loss=0.289, accuracy=0.92

Epoch 3/3:
  Step 700: loss=0.223, accuracy=0.94
  Step 800: loss=0.198, accuracy=0.95
  Step 900: loss=0.187, accuracy=0.96
  Validation: loss=0.245, accuracy=0.94

✅ 학습 완료!
최종 정확도: 94%
```

### 💾 모델 파일 변경

```
Before Fine-tuning:
models/kcbert/model.safetensors (420MB)
→ 일반적인 한국어 모델

After Fine-tuning:
models/kcbert-finetuned/
├─ model.safetensors (420MB)  ← 가중치가 변경됨!
├─ config.json                ← 설정 (3개 클래스)
├─ tokenizer_config.json
└─ training_args.bin          ← 학습 기록

차이점:
- 파일 크기: 동일 (420MB)
- 가중치 값: 변경됨! (특히 상위 레이어와 classifier)
- 성능: 크게 향상 (60% → 94%)
```

---

## 비유로 이해하기

### 🎓 대학 졸업생의 취업 교육

```
┌────────────────────────────────────────┐
│  사전 학습 (Pre-training)               │
│  = 대학 교육 (4년)                      │
├────────────────────────────────────────┤
│  - 한국어 문법, 어휘                    │
│  - 일반 상식, 교양                      │
│  - 기본 문해력                          │
│  💰 비용: 수천만원                      │
│  ⏱️  시간: 4년                         │
└────────────────────────────────────────┘
              ↓
┌────────────────────────────────────────┐
│  Fine-tuning                           │
│  = 직무 교육 (2주)                      │
├────────────────────────────────────────┤
│  - 고객센터 응대법                      │
│  - 욕설/성희롱 대응                     │
│  - 업무 프로세스                        │
│  💰 비용: 적음                          │
│  ⏱️  시간: 2주                         │
└────────────────────────────────────────┘
              ↓
┌────────────────────────────────────────┐
│  전문가                                 │
│  = 고객센터 전문 상담사                 │
├────────────────────────────────────────┤
│  ✅ 기본 지식 유지                      │
│  ✅ 전문 능력 추가                      │
│  ✅ 높은 업무 효율                      │
└────────────────────────────────────────┘
```

**핵심:**
- 대학 교육(사전 학습)을 버리지 않음 ✅
- 대학 교육 + 직무 교육(Fine-tuning) = 전문가
- 직무 교육만 하면 되니 시간/비용 절약!

---

## 요약

### 🎯 Fine-tuning이란?

```
✅ 사전 학습된 모델에 추가 학습을 시키는 것
✅ 모델의 가중치(weights)가 실제로 변경됨
✅ 기존 지식은 유지하면서 새로운 능력 추가
✅ 소량의 데이터로도 높은 정확도 달성
✅ 특정 작업에 특화된 모델 생성
```

### 📊 변경되는 것

| 항목 | 변경 여부 |
|-----|---------|
| 모델 파일 크기 | ❌ 동일 (420MB) |
| 모델 구조 | ❌ 동일 |
| 가중치 값 | ✅ **변경됨!** |
| 한국어 이해력 | ✅ 유지 |
| 욕설/성희롱 판단력 | ✅ **크게 향상!** |

### 💡 결론

**Fine-tuning = 모델을 특정 작업에 맞게 "재교육"하는 것**

```
일반 KcBERT (선생님)
    ↓ Fine-tuning (전문 교육)
욕설/성희롱 전문가 KcBERT

같은 사람이지만,
전문 지식이 추가되어
더 정확하고 똑똑해짐!
```

---

## 다음 단계

1. ✅ Fine-tuning 개념 이해 (현재)
2. ⏳ 학습 데이터 수집 및 라벨링
3. ⏳ Fine-tuning 코드 작성
4. ⏳ 모델 학습 실행
5. ⏳ 성능 평가 및 개선

---

## 참고 자료

- [Hugging Face Fine-tuning 가이드](https://huggingface.co/docs/transformers/training)
- [KcBERT GitHub](https://github.com/Beomi/KcBERT)
- [Transfer Learning 논문](https://arxiv.org/abs/1706.05137)
- [BERT Fine-tuning Tutorial](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)
