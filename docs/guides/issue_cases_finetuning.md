# 이슈 케이스 Fine-tuning 가이드

## 📅 작성일
2026-01-27

## 📋 개요

성능 테스트에서 발견된 이슈 케이스를 사용하여 KcBERT 모델을 Fine-tuning하고 성능을 개선합니다.

---

## 🎯 목적

### 문제점
초기 테스트 결과 KcBERT의 정확도가 **45%**로 매우 낮았습니다.

**주요 실패 케이스**:
```
❌ 모욕 표현 (욕설 없음): "쓰레기 같은 서비스" → 정상 판정
❌ 위협 표현: "죽이고 싶네" → 정상 판정
❌ 성희롱: "몸매 좋으시겠어요" → 정상 판정
❌ 비꼬기: "정말 대단하시네요" → 정상 판정
```

### 목표
Fine-tuning을 통해 **정확도 60% 이상** 달성

---

## 📊 이슈 케이스 분석

### 분류별 실패 케이스

#### 1. 모욕 표현 (욕설 없음)
```
test_04: "정말 무능한 사람들이네요. 쓰레기 같은 서비스."
  예상: 0.85 (부적절)
  실제: 0.38 (정상) ❌

test_08: "와 정말 대단하시네요. 진짜 훌륭하십니다." (비꼬기)
  예상: 0.70 (부적절)
  실제: 0.32 (정상) ❌

test_17: "한심하네요. 이래가지고 장사가 되나요?"
  예상: 0.60 (부적절)
  실제: 0.27 (정상) ❌
```

#### 2. 위협 표현
```
test_05: "가만 안 둔다. 죽이고 싶네 진짜."
  예상: 0.90 (부적절)
  실제: 0.37 (정상) ❌

test_18: "진짜 죽을래? 때려 죽이고 싶네."
  예상: 0.95 (부적절)
  실제: 0.32 (정상) ❌
```

#### 3. 성희롱
```
test_06: "남자친구 있으세요? 몸매도 좋으시겠어요."
  예상: 0.95 (부적절)
  실제: 0.29 (정상) ❌

test_07: "저녁 시간 되시나요? 같이 식사라도..."
  예상: 0.75 (부적절)
  실제: 0.37 (정상) ❌
```

#### 4. 수동공격적
```
test_16: "어차피 기대 안 했어요. 늘 이러시니까."
  예상: 0.65 (부적절)
  실제: 0.36 (정상) ❌
```

---

## 🔧 Fine-tuning 프로세스

### 1단계: 학습 데이터 준비

**파일**: `data/training/issue_cases_training.csv`

**구성**: 20개 케이스
```
- 부적절 (label=1): 11개
  - 모욕: 4개
  - 위협: 4개
  - 성희롱: 3개
  
- 정상/경계선 (label=0): 9개
  - 정상: 4개
  - 경계선: 5개
```

**데이터 형식**:
```csv
text,label,category,score,description
"정말 무능한 사람들이네요...",1,abusive,0.85,모욕적 표현
"안녕하세요. 배송 문의...",0,normal,0.10,정상 서비스 문의
```

### 2단계: Fine-tuning 실행

#### PowerShell 실행 (권장)
```powershell
.\run.ps1

# 메뉴 선택
6. 이슈 케이스 Fine-tuning 🔧

# 확인
Y
```

#### 직접 실행
```powershell
# 가상환경 활성화
.\venv\Scripts\Activate.ps1

# Fine-tuning 실행
python finetune_issue_cases.py
```

#### 처리 과정
```
1️⃣ 데이터 로드 (20개)
2️⃣ 데이터 분할 (80:20)
  ├─ 학습: 16개
  └─ 검증: 4개
3️⃣ 모델 로드 (beomi/kcbert-base)
4️⃣ 데이터셋 생성
5️⃣ 학습 설정
  ├─ 에폭: 10
  ├─ 배치: 4
  └─ Early Stopping
6️⃣ Fine-tuning (약 5-10분)
7️⃣ 모델 저장
  └─ models/kcbert-finetuned-issue-cases/
8️⃣ 평가
9️⃣ 결과 저장
```

### 3단계: 모델 평가

#### PowerShell 실행 (권장)
```powershell
.\run.ps1

# 메뉴 선택
7. Fine-tuned 모델 평가 📊
```

#### 직접 실행
```powershell
python evaluate_finetuned_model.py
```

#### 평가 내용
```
1️⃣ 원본 KcBERT 로딩
2️⃣ Fine-tuned KcBERT 로딩
3️⃣ 20개 테스트 케이스 실행
  ├─ 원본 모델로 테스트
  └─ Fine-tuned 모델로 테스트
4️⃣ 성능 비교
  ├─ 정확도
  ├─ MAE (점수 오차)
  └─ 처리 시간
5️⃣ 상세 비교표 생성
6️⃣ 결과 저장
```

---

## 📈 예상 결과

### 개선 목표

```
정확도:
  원본:      45% → 60%+ (목표 +15%p)
  Fine-tuned: 60%+ 달성

MAE (점수 오차):
  원본:      0.299 → 0.200 이하 (목표)
  Fine-tuned: 0.200 이하 달성

개선 케이스:
  11개 실패 케이스 중 8개+ 개선 (70%+)
```

### 케이스별 예상 개선

**모욕 표현** (4케이스)
```
Before: 0/4 감지 (0%)
After:  3/4 감지 (75%) 예상
```

**위협 표현** (4케이스)
```
Before: 0/4 감지 (0%)
After:  3/4 감지 (75%) 예상
```

**성희롱** (3케이스)
```
Before: 1/3 감지 (33%)
After:  2/3 감지 (67%) 예상
```

---

## 📁 생성 파일

### 학습 데이터
```
data/training/issue_cases_training.csv
└─ 20개 이슈 케이스 (CSV 형식)
```

### 스크립트
```
finetune_issue_cases.py
└─ Fine-tuning 스크립트

evaluate_finetuned_model.py
└─ 평가 스크립트
```

### Fine-tuned 모델
```
models/kcbert-finetuned-issue-cases/
├─ config.json
├─ pytorch_model.bin
├─ tokenizer_config.json
└─ vocab.txt
```

### 결과 파일
```
data/results/
├─ finetuning_result_YYYYMMDD_HHMMSS.json
└─ finetuned_evaluation_YYYYMMDD_HHMMSS.json
```

---

## 🎯 평가 결과 해석

### 결과 JSON 구조

```json
{
  "timestamp": "20260127_143052",
  "summary": {
    "original": {
      "accuracy": 45.0,
      "mae": 0.299
    },
    "finetuned": {
      "accuracy": 65.0,
      "mae": 0.215
    },
    "improvement": {
      "accuracy": 20.0,
      "mae": -0.084,
      "improved_cases": 14,
      "improved_percentage": 70.0
    }
  }
}
```

### 성공 기준

✅ **우수** (목표 초과 달성)
```
- 정확도 개선: +15%p 이상
- MAE 개선: -0.08 이상
- 개선 케이스: 70% 이상
```

⚠️ **양호** (목표 일부 달성)
```
- 정확도 개선: +5~15%p
- MAE 개선: -0.03~0.08
- 개선 케이스: 50~70%
```

❌ **미흡** (추가 조치 필요)
```
- 정확도 개선: +5%p 미만
- MAE 개선: -0.03 미만
- 개선 케이스: 50% 미만
```

---

## ⚠️ 주의사항

### 1. 데이터 부족
```
현재: 20개 케이스만 사용
권장: 100개 이상

→ 제한적 개선 예상
→ 추가 데이터 수집 필요
```

### 2. 과적합 위험
```
소량 데이터로 학습
→ 검증 데이터에만 잘 맞을 수 있음
→ Early Stopping으로 완화
```

### 3. 일반화 성능
```
이슈 케이스에만 최적화
→ 새로운 케이스에서는?
→ 지속적 평가 필요
```

### 4. 처리 시간
```
Fine-tuning: 5-10분
평가: 약 10초

→ 첫 실행 시 시간 소요
```

---

## 🔄 반복 개선 프로세스

### 1차 Fine-tuning
```
1. 이슈 케이스 20개로 학습
2. 평가 및 분석
3. 개선 효과 확인
```

### 2차 Fine-tuning (필요시)
```
1. 추가 실패 케이스 수집
2. 학습 데이터에 추가
3. 재학습 및 평가
```

### 지속적 개선
```
1. 실제 운영 데이터 수집
2. 정기적 재학습 (월 1회)
3. 성능 모니터링
4. 임계값 조정
```

---

## 💡 추가 개선 방안

### 단기 (1-2주)
```
✅ 이슈 케이스 Fine-tuning
✅ 평가 및 분석
✅ 규칙 기반 보완
```

### 중기 (1-2개월)
```
⏳ 실제 통화 데이터 100건+ 수집
⏳ 전문가 레이블링
⏳ 대규모 Fine-tuning
⏳ 정확도 75%+ 목표
```

### 장기 (3-6개월)
```
⏳ 1,000건+ 데이터 구축
⏳ 전용 모델 개발
⏳ 실시간 학습 시스템
⏳ 정확도 85%+ 목표
```

---

## 📊 실전 적용 시나리오

### 시나리오 1: Fine-tuned 모델 단독 사용
```
장점:
✅ 원본보다 개선된 성능
✅ 동일한 빠른 속도

단점:
❌ 여전히 60% 수준
❌ 40% 놓칠 수 있음

권장:
⚠️ 중요도 낮은 업무만
```

### 시나리오 2: Fine-tuned + 규칙 기반
```
장점:
✅ 정확도 70%+ 예상
✅ 빠른 속도 유지

단점:
⚠️ 규칙 관리 필요

권장:
✅ 실시간 모니터링
```

### 시나리오 3: Fine-tuned + sLLM (하이브리드)
```
1차: Fine-tuned KcBERT
  → 0.4 이상만 2차로

2차: sLLM 정밀 분석
  → 최종 판정

장점:
✅ 높은 정확도 (75%+)
✅ 속도와 정확도 균형

권장:
⭐ 최적의 접근 방식
```

---

## 📚 관련 문서

- `docs/PERFORMANCE_ANALYSIS_REPORT.md` - 초기 성능 분석
- `docs/guides/fine_tuning_explained.md` - Fine-tuning 상세 설명
- `docs/guides/accuracy_improvement.md` - 정확도 개선 방법
- `README.md` - 프로젝트 전체 개요

---

**작성일**: 2026-01-27  
**작성자**: AI Assistant  
**버전**: 1.0  
**상태**: ✅ 완료
