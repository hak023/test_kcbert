# -*- coding: utf-8 -*-
"""
ì´ìŠˆ ì¼€ì´ìŠ¤ Fine-tuning ìŠ¤í¬ë¦½íŠ¸
KcBERT ëª¨ë¸ì„ ì´ìŠˆ ì¼€ì´ìŠ¤ë¡œ ì¬í•™ìŠµ
"""

import sys
import os
import warnings
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import json
from datetime import datetime

warnings.filterwarnings('ignore')
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import logging
logging.getLogger('transformers').setLevel(logging.ERROR)


class AbusiveDataset(Dataset):
    """ìš•ì„¤/í­ì–¸ ê°ì§€ ë°ì´í„°ì…‹"""
    
    def __init__(self, texts, labels, tokenizer, max_length=300):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])
        
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


def compute_metrics(pred):
    """í‰ê°€ ì§€í‘œ ê³„ì‚°"""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='binary'
    )
    acc = accuracy_score(labels, preds)
    
    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }


def print_header(title):
    """í—¤ë” ì¶œë ¥"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70 + "\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print_header("ğŸ”§ ì´ìŠˆ ì¼€ì´ìŠ¤ Fine-tuning")
    
    print("ğŸ“ ì‘ì—… ê°œìš”")
    print("-" * 70)
    print("  â”œâ”€ ëª©ì : í…ŒìŠ¤íŠ¸ì—ì„œ ì‹¤íŒ¨í•œ ì´ìŠˆ ì¼€ì´ìŠ¤ë¡œ ëª¨ë¸ ê°œì„ ")
    print("  â”œâ”€ ë°ì´í„°: 20ê°œ ì´ìŠˆ ì¼€ì´ìŠ¤")
    print("  â”œâ”€ ë°©ë²•: KcBERT ëª¨ë¸ Fine-tuning")
    print("  â””â”€ í‰ê°€: ì •í™•ë„, Precision, Recall, F1")
    print()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print_header("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ")
    
    data_path = "data/training/issue_cases_training.csv"
    if not os.path.exists(data_path):
        print(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        return
    
    df = pd.read_csv(data_path)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")
    print()
    
    print("  ë°ì´í„° ë¶„í¬:")
    print(f"  â”œâ”€ ë¶€ì ì ˆ (label=1): {sum(df['label'] == 1)}ê°œ")
    print(f"  â””â”€ ì •ìƒ (label=0): {sum(df['label'] == 0)}ê°œ")
    print()
    
    # 2. ë°ì´í„° ë¶„í• 
    print_header("2ï¸âƒ£ ë°ì´í„° ë¶„í• ")
    
    # ë°ì´í„°ê°€ ì ìœ¼ë¯€ë¡œ 80:20 ë¶„í• 
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        df['text'].values,
        df['label'].values,
        test_size=0.2,
        random_state=42,
        stratify=df['label'].values
    )
    
    print(f"  â”œâ”€ í•™ìŠµ ë°ì´í„°: {len(train_texts)}ê°œ")
    print(f"  â””â”€ ê²€ì¦ ë°ì´í„°: {len(val_texts)}ê°œ")
    print()
    
    # 3. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
    print_header("3ï¸âƒ£ ëª¨ë¸ ë¡œë“œ")
    
    model_name = "beomi/kcbert-base"
    print(f"  ëª¨ë¸: {model_name}")
    print()
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
        problem_type="single_label_classification",
        ignore_mismatched_sizes=True
    )
    
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print()
    
    # 4. ë°ì´í„°ì…‹ ìƒì„±
    print_header("4ï¸âƒ£ ë°ì´í„°ì…‹ ìƒì„±")
    
    train_dataset = AbusiveDataset(train_texts, train_labels, tokenizer)
    val_dataset = AbusiveDataset(val_texts, val_labels, tokenizer)
    
    print(f"âœ… í•™ìŠµ ë°ì´í„°ì…‹: {len(train_dataset)}ê°œ")
    print(f"âœ… ê²€ì¦ ë°ì´í„°ì…‹: {len(val_dataset)}ê°œ")
    print()
    
    # 5. í•™ìŠµ ì„¤ì •
    print_header("5ï¸âƒ£ í•™ìŠµ ì„¤ì •")
    
    output_dir = "models/kcbert-finetuned-issue-cases"
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=10,  # ì†ŒëŸ‰ ë°ì´í„°ì´ë¯€ë¡œ ë§ì€ ì—í­
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        warmup_steps=50,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=10,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        save_total_limit=2,
        report_to="none"
    )
    
    print("  í•™ìŠµ ì„¤ì •:")
    print(f"  â”œâ”€ ì—í­: {training_args.num_train_epochs}")
    print(f"  â”œâ”€ ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}")
    print(f"  â”œâ”€ Learning Rate: {training_args.learning_rate}")
    print(f"  â””â”€ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print()
    
    # 6. Trainer ìƒì„±
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )
    
    # 7. Fine-tuning ì‹œì‘
    print_header("6ï¸âƒ£ Fine-tuning ì‹œì‘")
    
    print("â³ í•™ìŠµ ì¤‘... (ì•½ 5-10ë¶„ ì†Œìš”)")
    print()
    
    train_result = trainer.train()
    
    print()
    print("âœ… Fine-tuning ì™„ë£Œ!")
    print()
    
    # 8. ëª¨ë¸ ì €ì¥
    print_header("7ï¸âƒ£ ëª¨ë¸ ì €ì¥")
    
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")
    print()
    
    # 9. í‰ê°€
    print_header("8ï¸âƒ£ ëª¨ë¸ í‰ê°€")
    
    eval_result = trainer.evaluate()
    
    print("  ê²€ì¦ ë°ì´í„° í‰ê°€ ê²°ê³¼:")
    print(f"  â”œâ”€ Accuracy:  {eval_result['eval_accuracy']:.4f}")
    print(f"  â”œâ”€ Precision: {eval_result['eval_precision']:.4f}")
    print(f"  â”œâ”€ Recall:    {eval_result['eval_recall']:.4f}")
    print(f"  â””â”€ F1 Score:  {eval_result['eval_f1']:.4f}")
    print()
    
    # 10. í•™ìŠµ ê¸°ë¡ ì €ì¥
    print_header("9ï¸âƒ£ í•™ìŠµ ê¸°ë¡ ì €ì¥")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = f"data/results/finetuning_result_{timestamp}.json"
    
    result = {
        "timestamp": timestamp,
        "model": model_name,
        "output_dir": output_dir,
        "training_data_size": len(train_texts),
        "validation_data_size": len(val_texts),
        "epochs": training_args.num_train_epochs,
        "batch_size": training_args.per_device_train_batch_size,
        "train_result": {
            "train_loss": float(train_result.training_loss),
            "train_runtime": train_result.metrics['train_runtime'],
            "train_samples_per_second": train_result.metrics['train_samples_per_second']
        },
        "eval_result": {
            "accuracy": float(eval_result['eval_accuracy']),
            "precision": float(eval_result['eval_precision']),
            "recall": float(eval_result['eval_recall']),
            "f1": float(eval_result['eval_f1']),
            "loss": float(eval_result['eval_loss'])
        }
    }
    
    os.makedirs("data/results", exist_ok=True)
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… í•™ìŠµ ê¸°ë¡ ì €ì¥: {result_file}")
    print()
    
    # 11. ìµœì¢… ìš”ì•½
    print_header("ğŸ¯ Fine-tuning ì™„ë£Œ")
    
    print("  ğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"  â”œâ”€ ì •í™•ë„: {eval_result['eval_accuracy']*100:.1f}%")
    print(f"  â”œâ”€ F1 Score: {eval_result['eval_f1']:.4f}")
    print(f"  â””â”€ ëª¨ë¸ ìœ„ì¹˜: {output_dir}")
    print()
    
    print("  ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    print("  â”œâ”€ 1. Fine-tuned ëª¨ë¸ë¡œ 20ê°œ ì¼€ì´ìŠ¤ ì¬í‰ê°€")
    print("  â”œâ”€ 2. ê°œì„  íš¨ê³¼ í™•ì¸")
    print("  â””â”€ 3. í•„ìš”ì‹œ ì¶”ê°€ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘")
    print()
    
    print("=" * 70)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
