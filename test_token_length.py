# -*- coding: utf-8 -*-
"""
KcBERT í† í° ê¸¸ì´ í…ŒìŠ¤íŠ¸
300 í† í°ì´ ì‹¤ì œë¡œ ì–¼ë§ˆë‚˜ ë˜ëŠ”ì§€ í™•ì¸
"""

import sys
import os
import warnings

warnings.filterwarnings('ignore')
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import logging
logging.getLogger('transformers').setLevel(logging.ERROR)


def test_token_length():
    """í† í° ê¸¸ì´ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 70)
    print("ğŸ”¬ KcBERT í† í° ê¸¸ì´ í…ŒìŠ¤íŠ¸")
    print("=" * 70 + "\n")
    
    # KcBERT í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("ğŸ“¥ KcBERT í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    from transformers import AutoTokenizer
    
    tokenizer = AutoTokenizer.from_pretrained(
        "beomi/kcbert-base",
        cache_dir="./models/kcbert"
    )
    print("âœ… ë¡œë”© ì™„ë£Œ\n")
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë“¤
    test_cases = [
        # 1. ì§§ì€ ë¬¸ì¥
        {
            "name": "ì§§ì€ ë¬¸ì¥",
            "text": "ì•ˆë…•í•˜ì„¸ìš”. ì œí’ˆ ë¬¸ì˜ ë“œë¦½ë‹ˆë‹¤."
        },
        # 2. ë³´í†µ ê¸¸ì´ (ìƒ˜í”Œ ë°ì´í„°)
        {
            "name": "ë³´í†µ ê¸¸ì´ (normal_call)",
            "text": """ê³ ê°: ì•ˆë…•í•˜ì„¸ìš”, ì œí’ˆ ë¬¸ì˜ ë“œë¦½ë‹ˆë‹¤.
ìƒë‹´ì›: ë„¤ ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?
ê³ ê°: A ìƒí’ˆì˜ ë°°ì†¡ ê¸°ê°„ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤.
ìƒë‹´ì›: A ìƒí’ˆì€ ì£¼ë¬¸ í›„ 2-3ì¼ ì´ë‚´ì— ë°°ì†¡ë©ë‹ˆë‹¤.
ê³ ê°: ê·¸ë ‡êµ°ìš”. ë°˜í’ˆë„ ê°€ëŠ¥í•œê°€ìš”?
ìƒë‹´ì›: ë„¤, ìˆ˜ë ¹ í›„ 7ì¼ ì´ë‚´ ë¯¸ê°œë´‰ ìƒíƒœë©´ ë°˜í’ˆ ê°€ëŠ¥í•©ë‹ˆë‹¤.
ê³ ê°: ì•Œê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤. ë„ì›€ì´ ë˜ì—ˆì–´ìš”.
ìƒë‹´ì›: ì²œë§Œì—ìš”. ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!"""
        },
        # 3. ê¸´ ë¬¸ì¥
        {
            "name": "ê¸´ í†µí™” (mixed_call)",
            "text": """ê³ ê°: ì•ˆë…•í•˜ì„¸ìš”. ì£¼ë¬¸í•œ ìƒí’ˆì´ ì•„ì§ ì•ˆ ì™”ëŠ”ë°ìš”.
ìƒë‹´ì›: ë„¤, ì£¼ë¬¸ë²ˆí˜¸ë¥¼ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?
ê³ ê°: 12345678ì…ë‹ˆë‹¤.
ìƒë‹´ì›: í™•ì¸í•´ë³´ë‹ˆ í˜„ì¬ ë°°ì†¡ ì¤‘ì´ë©°, ë‚´ì¼ ë„ì°© ì˜ˆì •ì…ë‹ˆë‹¤.
ê³ ê°: ì•„ë‹ˆ ì´ë¯¸ ì¼ì£¼ì¼ì´ ì§€ë‚¬ëŠ”ë° ë‚´ì¼ì´ìš”? ì§„ì§œ ë„ˆë¬´í•œ ê±° ì•„ë‹™ë‹ˆê¹Œ?
ìƒë‹´ì›: ì£„ì†¡í•©ë‹ˆë‹¤. íƒë°°ì‚¬ ì‚¬ì •ìœ¼ë¡œ ì§€ì—°ë˜ì—ˆìŠµë‹ˆë‹¤.
ê³ ê°: ì—íœ´, ë‹µë‹µí•˜ë„¤. ì´ëŸ´ ê±°ë©´ ë‹¤ë¥¸ ë°ì„œ ì‚´ ê±¸ ê·¸ë¬ì–´.
ìƒë‹´ì›: ì •ë§ ì£„ì†¡í•©ë‹ˆë‹¤. ë°°ì†¡ë¹„ëŠ” í™˜ë¶ˆí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
ê³ ê°: ê·¸ë˜ìš”, ë‹¤ìŒì—” ì œë°œ ì œë•Œ ë³´ë‚´ì£¼ì„¸ìš”.
ìƒë‹´ì›: ë„¤, ì•ìœ¼ë¡œ ë”ìš± ì‹ ê²½ ì“°ê² ìŠµë‹ˆë‹¤. ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤."""
        },
        # 4. 300 í† í° ê·¼ì²˜ í…ìŠ¤íŠ¸
        {
            "name": "300 í† í° í…ŒìŠ¤íŠ¸",
            "text": "ì•ˆë…•í•˜ì„¸ìš”. " * 100  # ë°˜ë³µ í…ìŠ¤íŠ¸
        },
    ]
    
    print("â”€" * 70)
    print()
    
    # ê° ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
    for i, case in enumerate(test_cases, 1):
        text = case['text']
        
        # í† í°í™”
        tokens = tokenizer.encode(text, add_special_tokens=True)
        token_count = len(tokens)
        
        # ë¬¸ì/ë‹¨ì–´ ìˆ˜
        char_count = len(text)
        word_count = len(text.split())
        
        # 300 í† í° ëŒ€ë¹„ ë¹„ìœ¨
        ratio = (token_count / 300) * 100
        
        print(f"[í…ŒìŠ¤íŠ¸ {i}] {case['name']}")
        print(f"  ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {char_count}ì")
        print(f"  ğŸ“„ ë‹¨ì–´ ìˆ˜: {word_count}ê°œ")
        print(f"  ğŸ”¢ í† í° ìˆ˜: {token_count}ê°œ")
        print(f"  ğŸ“Š 300 í† í° ëŒ€ë¹„: {ratio:.1f}%")
        
        if token_count > 300:
            print(f"  âš ï¸  ê²½ê³ : 300 í† í° ì´ˆê³¼! ({token_count - 300}ê°œ ì´ˆê³¼)")
            print(f"  âœ‚ï¸  ì˜ë¦¼: ë’·ë¶€ë¶„ {token_count - 300}ê°œ í† í°ì´ ì˜ë¦½ë‹ˆë‹¤")
        else:
            print(f"  âœ… OK: 300 í† í° ì´ë‚´")
        
        # í† í°ë‹¹ í‰ê·  ê¸€ì ìˆ˜
        if token_count > 0:
            char_per_token = char_count / token_count
            print(f"  ğŸ“ í† í°ë‹¹ í‰ê· : {char_per_token:.2f}ì/í† í°")
        
        # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
        if len(text) > 100:
            preview = text[:100] + "..."
        else:
            preview = text
        print(f"  ğŸ’¬ ë¯¸ë¦¬ë³´ê¸°: \"{preview}\"")
        print()
    
    print("â”€" * 70)
    print()
    
    # 300 í† í° ì‹¤ì œ í¬ê¸° ê³„ì‚°
    print("ğŸ“Š 300 í† í° = ì‹¤ì œ í¬ê¸° ì¶”ì •:")
    print()
    
    # í‰ê· ê°’ ê³„ì‚°
    test_text = """ì¼ë°˜ì ì¸ ê³ ê°ì„¼í„° í†µí™” ë‚´ìš©ì…ë‹ˆë‹¤. 
ê³ ê°ì´ ë¬¸ì˜ë¥¼ í•˜ë©´ ìƒë‹´ì›ì´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
ì œí’ˆì— ëŒ€í•œ ì„¤ëª…, ë°°ì†¡ ì •ë³´, í™˜ë¶ˆ ì ˆì°¨ ë“±ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.
ê³ ê°ì˜ ë¶ˆí¸ì‚¬í•­ì„ ê²½ì²­í•˜ê³  ìµœì„ ì„ ë‹¤í•´ í•´ê²°ì±…ì„ ì œì‹œí•©ë‹ˆë‹¤."""
    
    tokens = tokenizer.encode(test_text)
    avg_char_per_token = len(test_text) / len(tokens)
    
    estimated_chars_300 = 300 * avg_char_per_token
    estimated_words_300 = estimated_chars_300 / 5  # í•œêµ­ì–´ í‰ê·  ë‹¨ì–´ ê¸¸ì´ ì•½ 5ì
    
    print(f"  â€¢ í•œêµ­ì–´ í‰ê· : 1 í† í° â‰ˆ {avg_char_per_token:.2f}ì")
    print(f"  â€¢ 300 í† í° â‰ˆ {estimated_chars_300:.0f}ì")
    print(f"  â€¢ 300 í† í° â‰ˆ {estimated_words_300:.0f}ê°œ ë‹¨ì–´")
    print(f"  â€¢ 300 í† í° â‰ˆ A4 ìš©ì§€ ì•½ 0.5í˜ì´ì§€")
    print()
    
    # ì‹¤ì œ ì˜ˆì œ
    print("ğŸ’¡ ì‹¤ì œ ì˜ˆì œ:")
    print()
    print("  ì§§ì€ í†µí™” (1-2ë¶„): 100-150 í† í° ì •ë„")
    print("  ë³´í†µ í†µí™” (3-5ë¶„): 200-300 í† í° ì •ë„")
    print("  ê¸´ í†µí™” (5ë¶„+):   300+ í† í° (ì˜ë¦¼ ë°œìƒ)")
    print()
    
    print("â”€" * 70)
    print()
    
    # 300 í† í° ì´ˆê³¼ ì‹œ ëŒ€ì²˜ë²•
    print("âš ï¸  300 í† í° ì´ˆê³¼ ì‹œ ëŒ€ì²˜ë²•:")
    print()
    print("  1. ìë™ íŠ¸ëŸ°ì¼€ì´ì…˜ (í˜„ì¬ ë°©ì‹)")
    print("     - ë’·ë¶€ë¶„ì´ ì˜ë¦½ë‹ˆë‹¤")
    print("     - ì•ë¶€ë¶„ 300 í† í°ë§Œ ë¶„ì„")
    print()
    print("  2. ë¶„í•  ì²˜ë¦¬ (êµ¬í˜„ í•„ìš”)")
    print("     - í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ”")
    print("     - ê° ì¡°ê°ì„ ê°œë³„ ë¶„ì„")
    print("     - ê²°ê³¼ë¥¼ ì¢…í•©")
    print()
    print("  3. ìš”ì•½ í›„ ì²˜ë¦¬ (êµ¬í˜„ í•„ìš”)")
    print("     - ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½")
    print("     - ìš”ì•½ë³¸ì„ ë¶„ì„")
    print()
    
    print("=" * 70)
    
    # í† í° ì‹œê°í™”
    print()
    print("ğŸ” í† í° ì‹œê°í™” ì˜ˆì œ:")
    print()
    
    sample = "ê³ ê°: ì•ˆë…•í•˜ì„¸ìš”. ë°°ì†¡ì´ ëŠ¦ì–´ì ¸ì„œ ì—°ë½ë“œë¦½ë‹ˆë‹¤."
    tokens = tokenizer.tokenize(sample)
    token_ids = tokenizer.encode(sample)
    
    print(f"  ì›ë¬¸: \"{sample}\"")
    print(f"  í† í° ìˆ˜: {len(tokens)}ê°œ")
    print(f"  í† í°ë“¤: {tokens}")
    print(f"  í† í° ID: {token_ids}")
    print()
    
    print("=" * 70)


if __name__ == "__main__":
    test_token_length()
